print("hello")
install.packages(c("nlme", "survival"))
print("hello")
library(ggplot2)
library(ggplot)
source('~/.active-rstudio-document')
source('~/.active-rstudio-document')
library(ggplot2)
library(ggplot2)
knitr::opts_chunk$set(echo = TRUE)
print("this in a r code chunk")
> print("this in a r code chunk")
print("this in a r code chunk")
plot(pressure)
library(rmarkdown)
render("/path/to/hello-Rmd.Rmd")
print("this is a r code chunk")
R
library(markdown)
library(rmarkdown)
{r}
nile <- scan("nile.txt")
nile
{r}
nile <- scan("nile.txt")
nile
nile <- read.table("~/Downloads/nile.txt", quote="\"", comment.char="")
View(nile)
{r}
nile <- scan("nile.txt")
nile
{r}
typeof(nile)
{r}
typeof(nile_char)
{r}
hist(nile)
{r}
nile_char <- scan("nile.txt", what = character())
nile_char
{r}
nile <- scan("nile.txt")
nile
{r}
typeof(nile)
{r}
hist(nile)
{r}
nile_char <- scan("nile.txt", what = character())
nile_char
{r}
typeof(nile_char)
{r}
nile <- scan("nile.txt")
nile
{r}
typeof(nile)
{r}
hist(nile)
{r}
nile <- scan("nile.txt")
nile
{r}
typeof(nile)
{r}
nile <- scan("nile.txt")
nile
{}
{r}
typeof(nile)
{r}
nile <- scan("nile.txt")
nile
{}
{r}
typeof(nile)
{}
{r}
nile <- scan("nile.txt")
nile
{}
{r}
typeof(nile)
{}
knitr::opts_chunk$set(echo = TRUE)
```{r arithmetic}
install.packages("xml2")
install.packages("xml2")
library(xml2)
library(rvest)
install.packages("dplyr")
install.packages("htmlTable")
url <- "https://en.wikipedia.org/wiki/Comma-separated_values"
html <- read_html(url)
name <- html %>%
html_nodes(xpath = '//*[@id="mw-content-text"]/div[1]/table[2])
html_table()
print(htmlTable::htmlTable(name,useViewer=TRUE))
write.csv(name, file = "table.csv", row.names = F)
(df <- read.csv("table.csv"))
library(rvest)
install.packages("dplyr")
install.packages("htmlTable")
url <- "https://en.wikipedia.org/wiki/Comma-separated_values"
html <- read_html(url)
name <- html %>%
html_nodes(xpath = '//*[@id="mw-content-text"]/div[1]/table[2])
html_table()
print(htmlTable::htmlTable(name,useViewer=TRUE))
write.csv(name, file = "table.csv", row.names = F)
(df <- read.csv("table.csv"))
name
library(rvest)
install.packages("dplyr")
install.packages("htmlTable")
url <- "https://en.wikipedia.org/wiki/Comma-separated_values"
html <- read_html(url)
name <- html %>%
html_nodes(xpath = '//*[@id="mw-content-text"]/div[1]/table[2])
html_table()
print(htmlTable::htmlTable(name,useViewer=TRUE))
write.csv(name, file = "table.csv", row.names = F)
(df <- read.csv("table.csv"))
print("name")
install.packages("htmlTable")
library(rvest)
library(dbplyr)
library(htmlTable)
link <- "https://en.wikipedia.org/wiki/Comma-separated_values"
page <- read_html(link)
name <- page%>%
html_nodes(xpath = '//*[@id="mw-content-text"]/div[1]/table[2]')%>%
html_table()
print(htmlTable: htmlTable(name,useViewer=TRUE))
write.csv(name, file = "table.csv", row.names = F)
(df <- read.csv("table.csv"))
library(rvest)
library(dbplyr)
library(htmlTable)
link <- "https://en.wikipedia.org/wiki/Comma-separated_values"
page <- read_html(link)
name <- page%>%
html_nodes(xpath = '//*[@id="mw-content-text"]/div[1]/table[2]')%>%
html_table()
print(htmlTable: htmlTable(name,useViewer=TRUE))
write.csv(name, file = "table.csv", row.names = F)
(df <- read.csv("table.csv"))
write.csv(df,"C:\\Users\\samanthatricia@samanthas-mbp\\Desktop\\Assignment-2\\r-csv", row.names = FALSE)
library(rvest)
library(dbplyr)
library(htmlTable)
link <- "https://en.wikipedia.org/wiki/Comma-separated_values"
page <- read_html(link)
name <- page%>%
html_nodes(xpath = '//*[@id="mw-content-text"]/div[1]/table[2]')%>%
html_table()
print(htmlTable: htmlTable(name,useViewer=TRUE))
write.csv(name, file = "table.csv", row.names = F)
(df <- read.csv("table.csv"))
write.csv(df,"Path to export the DataFrame\\r_csv.csv", row.names = FALSE)
library(rvest)
library(dbplyr)
library(htmlTable)
link <- "https://en.wikipedia.org/wiki/Comma-separated_values"
page <- read_html(link)
name <- page%>%
html_nodes(xpath = '//*[@id="mw-content-text"]/div[1]/table[2]')%>%
html_table()
print(htmlTable: htmlTable(name,useViewer=TRUE))
write.csv(name, file = "table.csv", row.names = F)
(df <- read.csv("table.csv"))
library(rvest)
library(dbplyr)
library(htmlTable)
link <- "https://en.wikipedia.org/wiki/Comma-separated_values"
page <- read_html(link)
name <- page%>%
html_nodes(xpath = '//*[@id="mw-content-text"]/div[1]/table[2]')%>%
html_table()
print(htmlTable: htmlTable(name,useViewer=TRUE))
write.csv(name, file = "table.csv", row.names = F)
(df <- read.csv("table.csv"))
#Radius
r<-2
#Function to compute the volume of a sphere with radius r
volume <- function(r,rho){
3/4*pi*r^2
}
#Function to compute the volumes of the spheres with radius r,r^2 and r^3
volume_vector <- function(r){
r<-22
for (r in 2:4){
volume(r)
}
}
#Run volume_vector(r) and print the volumes of the spheres with radius r,r^2 and r^3
volume_vector(r)
#Radius
r<-2
#Function to compute the volume of a sphere with radius r
volume <- function(r,rho){
3/4*pi*r^2
}
#Function to compute the volumes of the spheres with radius r, r^2 and r^3
volume_vector <- function(r){
r<-22
for (r in 2:4){
volume(r)
}
}
#Run volume_vector(r) and print the volumes of the spheres with radius r, r^2 and r^3
volume_vector(r)
def is_divisible_by_k(x, k):
'''
Checks whether x is divisible by k.
'''
assert x%k == 0
'''
Store all the integers that are multiples of 2 or 5 or 7 that are lower or equal to 1000 (excluding
doubles)
'''
x = ()
for i in range(1000):
if (is_divisible_by_k(x, 2) & is_divisible_by_k(x, 3)) | is_divisible_by_k(x, 7):
x.append(i)
'''
Sum all the integers that are multiples of 2 or 5 or 7 that are lower or equal to 1000 (excluding
doubles)
'''
sum(x)
setwd("~/Desktop/SIM/ST2195/prac/st2195_Assignment_6")
library(dplyr)
fx <- read.csv("~/Desktop/SIM/ST2195/prac/st2195_Assignment_6/fx.csv", skip=6, header = TRUE, na.strings = "-")
#missing values stored as "-"
fx <- fx[1:2]
#subset the first 2 columns
colnames(fx) <- c("date", "exchange_rate")
str(fx)
nrow(fx)
length(unique(fx$date))
head(fx)
speeches <- read.csv("~/Desktop/SIM/ST2195/prac/st2195_Assignment_6/speeches.csv", sep = '|', quote ="", encoding ="UTF-8")
speeches <- speeches[!is.na(speeches$contents),c('date', 'contents')]
#is.na --- filter out the null datas
str(speeches)
nrow(speeches)
length(unique(speeches$date)) #there can be more than one speeches on the same date
head(fx)
speeches <- speeches %>%
group_by(date) %>%
summarise(contents=paste(contents,collapse = " "))
str(speeches)
nrow(speeches)
# syntax:left.join(x, y, by = NULL, copy = FALSE, suffix = c(".x", ".y"), ...)
df <- fx %>% left_join(speeches)
str(df)
df$exchange_rate <- as.numeric(df$exchange_rate)
df$date <- as.Date(df$date)
str(df)
## REMOVE ENTRIES WITH OBVIOUS OUTLIERS OR MISTAKES
# Check for any obvious outliers or mistakes by plotting the data
plot(df$date, df$exchange_rate, type ='l', xlab="date",
ylab = "EUR/USD reference exchange rate")
summary(df)
## HANDLE MISSING DATA
#we can use fill from dplyr package(part of tidyverse) also use the 'na.locf()' from zoo
#syntax is fill(data, ...., .direction = c("down", "downup", "updown"))
#Use fill direction of "up" as date in descending order and we want prior order value
install.packages("tidyr")
library(tidyr)
df2 <- df
df2 <- df2 %>% fill(exchange_rate, .direction ="up")
summary(df2)
diff(df2$exchange_rate)
#as date is in descending order, multiply by -1?
#depends on perpective? EURO? USD?
diff(df2$exchange_rate)*(-1)
df2$exchange_rate
df2$exchange_rate[-1] #exclude the first element
#last element set to NA, as there aren't any further prior values for calculation
df2$return <- c( (diff(df2$exchange_rate)*(-1)) / df2$exchange_rate[-1], NA)
df2$good_news <- as.numeric(df2$return > 0.5/100)
df2$bad_news <- as.numeric(df2$return > -0.5/100)
library(tidyr)
df2 <- df2 %>% drop_na(contents)
str(df2)
good_news_contents <- df2$contents[df2$good_news==1]
bad_news_contents <- df2$contents[df2$bad_news==1]
str(bad_news_contents)
#Load in stop words, which are those used to form a sentence but does not add much meaning.
stop_words <- read.csv("~/Desktop/SIM/ST2195/prac/stop_words_english.txt", header = FALSE)[,1]
stop_words
install.packages("text2vec")
library(text2vec)
??text2vec
get_word_freq <- function(contents, stop_words, num_words) {
# turn a paragraph to a vector of words
words <- unlist(lapply(contents, word_tokenizer))
# turn all words to lowercase
words <- tolower(words)
# find out the number of appearance of each word
freq <- table(words)
#length(freq)
# remove the stop words
freq <- freq[!(names(freq) %in% stop_words)]
#length(freq)
# sort the words from appearing most to least and return the result
freq <- sort(freq, decreasing=TRUE)
return(names(freq)[1:num_words])
}
good_indicators <- get_word_freq(good_news_contents, stop_words, num_words = 20)
good_indicators
bad_indicators <- get_word_freq(bad_news_contents, stop_words, num_words = 20)
bad_indicators
install.packages("textstem")
library(textstem)
#add lemmatization step to the word counter function
get_word_freq_lemmatize <- function(contents, stop_words, num_words) {
# turn a paragraph to a vector of words
words <- unlist(lapply(contents, word_tokenizer))
# turn all words to lowercase
words <- tolower(words)
# lemmatize words
words <- lemmatize_words(words)
# find out the number of appearance of each word
freq <- table(words)
# remove the stop words
freq <- freq[!(names(freq) %in% stop_words)]
# sort the words from appearing most to least and return result
freq <- sort(freq, decreasing=TRUE)
return(names(freq)[1:num_words])
#names(freq[order(-freq)])[1:num_words] #another way to do this
}
good_indicators2 <- get_word_freq_lemmatize(good_news_contents, stop_words, num_words=20)
good_indicators2
bad_indicators2 <- get_word_freq_lemmatize(bad_news_contents, stop_words, num_words=20)
bad_indicators2
